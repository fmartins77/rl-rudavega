{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiE7sag3+sYDL1a+00nUyF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbJKf7Q6hmBw","executionInfo":{"status":"ok","timestamp":1683820592275,"user_tz":240,"elapsed":21872,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"5512e534-7358-4721-f4b7-644b32033c1e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sC5KagUeiAc6","executionInfo":{"status":"ok","timestamp":1683821107997,"user_tz":240,"elapsed":132,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"07e11533-67c5-487e-ec35-a6526fc83778"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jH1GarCdiOMF","executionInfo":{"status":"ok","timestamp":1683821111126,"user_tz":240,"elapsed":393,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"7c66ebbc-c1ca-4f63-88ff-b6d051de9497"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["!git init reinforcement-learning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzaBXKA3jqVf","executionInfo":{"status":"ok","timestamp":1683821125405,"user_tz":240,"elapsed":517,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"602726ff-213a-4083-bac4-ca0ee91fbdec"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/drive/MyDrive/reinforcement-learning/.git/\n"]}]},{"cell_type":"code","source":["%ls -a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEpSddPlj3KU","executionInfo":{"status":"ok","timestamp":1683821139175,"user_tz":240,"elapsed":280,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"35f110cf-66a7-4eb3-d2c8-e41975639b6c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/           \u001b[01;34m'Markov Decision Processes'\u001b[0m/\n"," \u001b[01;34mdeep-learning-python-book\u001b[0m/   \u001b[01;34mPNS\u001b[0m/\n"," \u001b[01;34mdynamical-systems\u001b[0m/           \u001b[01;34mreinforcement-learning\u001b[0m/\n"]}]},{"cell_type":"code","source":["%cd reinforcement-learning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsUT4kjlj29k","executionInfo":{"status":"ok","timestamp":1683821155796,"user_tz":240,"elapsed":107,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"cbc066aa-6973-4f17-c07d-ab6e7eef6889"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/reinforcement-learning\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SsIh_1aAj-wP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5TkPg0xqj-tH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_k9wY6ubj-qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"icCCmKIOj-n6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xwTSMABuj-lS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EAL27arMj-is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_hwspQ0j-f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cBZUC4VRj-dP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTOql9tCOlxh"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# A/B/n Testing"],"metadata":{"id":"sks4uVLfREoB"}},{"cell_type":"markdown","source":["We first create a function factory that will create a random number generator that generates normal variates with a given mean and standard deviation:"],"metadata":{"id":"VaI1SmZafGSm"}},{"cell_type":"code","source":["def normal_generator(mean, stdev, rng):\n","    def generator():\n","        return rng.normal(mean, stdev)\n","    return generator"],"metadata":{"id":"MxbUU6H-RvJx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now implement A/B/n Testing.\n","\n","We first simulate the game choosing a random action for a certain number of steps."],"metadata":{"id":"-dNM2uapU8cR"}},{"cell_type":"code","source":["# Restart random number generator\n","rng = np.random.default_rng(seed=42)  \n","\n","# Define array of actions\n","nactions = 5\n","means = [1.1, 2.3, 2.2, 1.2, 2.1]\n","stdevs = [1, 1, 1, 1, 1]\n","reward_generators = [normal_generator(mean, stdev, rng) for mean, stdev in zip(means, stdevs)]\n","\n","# Initialize simulation\n","nsteps = 1000\n","action_values = np.zeros(nactions, dtype=np.float64)\n","action_counts = np.zeros(nactions, dtype=np.int64)\n","\n","# Run simulation\n","for _ in range(nsteps):\n","    action = rng.integers(nactions)\n","    reward = reward_generators[action]()\n","    action_counts[action] += 1\n","    action_values[action] += (reward - action_values[action]) / action_counts[action]\n","print('Training results')\n","for i in range(nactions):\n","  print(f'Action {i}: Count: {action_counts[i]} Value: {action_values[i]}')\n","best_action = np.argmax(action_values)\n","best_value = action_values[best_action]\n","print(f'Best action: {best_action} Value: {best_value}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aglKcajRUBSe","executionInfo":{"status":"ok","timestamp":1683757024117,"user_tz":240,"elapsed":126,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"fa02e891-1ef3-474a-8759-8401d4bd7737"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training results\n","Action 0: Count: 198 Value: 1.0794119031506175\n","Action 1: Count: 204 Value: 2.271695036128357\n","Action 2: Count: 204 Value: 2.1292896561988575\n","Action 3: Count: 201 Value: 1.154292804913939\n","Action 4: Count: 193 Value: 2.1130371998782436\n","Best action: 1 Value: 2.271695036128357\n"]}]},{"cell_type":"markdown","source":["Things to experiment with:\n","\n","- Run the code above with different values of the seed. Is there variation on the best action selected?\n","\n","- Run the code with different values of the means and standard deviation.\n","\n","- Is there a minimum number of steps that reliably selects the best action for any values of the parameters?\n","\n","(It is possible to give a precise answer to this question, depending on the values of the standard deviations. A way to get a feeling for that experimentally would be to compare only two actions with close means and different standard deviations)"],"metadata":{"id":"CPWS6eRIf4Ky"}},{"cell_type":"markdown","source":["# $\\epsilon$ Greedy Actions"],"metadata":{"id":"0dMaJ6dwt3Q7"}},{"cell_type":"code","source":["# Restart random number generator\n","rng = np.random.default_rng(seed=2048)  \n","\n","# Define array of actions\n","nactions = 5\n","means = [1.1, 2.3, 2.2, 1.2, 2.1]\n","stdevs = [1, 1, 1, 1, 1]\n","reward_generators = [normal_generator(mean, stdev, rng) for mean, stdev in zip(means, stdevs)]\n","\n","# Initialize simulation\n","eps = 0.001\n","nsteps = 10000\n","action_values = np.zeros(nactions, dtype=np.float64)\n","action_counts = np.zeros(nactions, dtype=np.int64)\n","\n","# Run simulation\n","for _ in range(nsteps):\n","    if rng.random() < eps:\n","        action = rng.integers(nactions)\n","    else:\n","        action = np.argmax(action_values)\n","    reward = reward_generators[action]()\n","    action_counts[action] += 1\n","    action_values[action] += (reward - action_values[action]) / action_counts[action]\n","print('Training results')\n","for i in range(nactions):\n","  print(f'Action {i}: Count: {action_counts[i]} Value: {action_values[i]}')\n","best_action = np.argmax(action_values)\n","best_value = action_values[best_action]\n","print(f'Best action: {best_action} Value: {best_value}')\n"],"metadata":{"id":"4dgeYTmbbC_k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683757303658,"user_tz":240,"elapsed":261,"user":{"displayName":"Luiz Felipe Martins","userId":"02488179495456249327"}},"outputId":"f99e3f74-a6d5-45aa-f062-4a1190f61de0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training results\n","Action 0: Count: 5 Value: 0.33055188588313\n","Action 1: Count: 9989 Value: 2.2861703712256958\n","Action 2: Count: 5 Value: 2.112255655820844\n","Action 3: Count: 1 Value: 1.2547221814541167\n","Action 4: Count: 0 Value: 0.0\n","Best action: 1 Value: 2.2861703712256958\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Rn5lf-ILurMs"},"execution_count":null,"outputs":[]}]}